{
  "id": "5bc32a5ae8eb6400138a6d5a",
  "title": "Transformer",
  "created": 1539517021,
  "updated": 1539612307,
  "lines": [
    {
      "id": "5bc32a5ae8eb6400138a6d5a",
      "text": "Transformer",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539517021,
      "updated": 1539517022
    },
    {
      "id": "5bc49a6daff09e00008770ca",
      "text": "[RNN]なし[CNN]なしで[注意機構]だけ構成されたTransformerが翻訳タスクで良い成績を出すという報告。",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539611245,
      "updated": 1539611603
    },
    {
      "id": "5bc49a6daff09e00008770cb",
      "text": ">We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. ",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539611245,
      "updated": 1539611245
    },
    {
      "id": "5bc49aa2aff09e00008770cd",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539611299,
      "updated": 1539611299
    },
    {
      "id": "5bc49a36aff09e00008770c9",
      "text": "\tAttention Is All You Need [Łukasz Kaiser et al., arXiv, 2017/06]",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539611190,
      "updated": 1539611311
    },
    {
      "id": "5bc48713aff09e0000dab5e6",
      "text": "\t2017年",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539606291,
      "updated": 1539611310
    },
    {
      "id": "5bc49aacaff09e00008770ce",
      "text": "\thttps://arxiv.org/pdf/1706.03762.pdf ",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539611308,
      "updated": 1539612307
    },
    {
      "id": "5bc48713aff09e0000dab5e7",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539606291,
      "updated": 1539611244
    },
    {
      "id": "5bc32ac1aff09e000044c690",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539517121,
      "updated": 1539517121
    },
    {
      "id": "5bc32a72aff09e000044c68d",
      "text": "解説 2017-12 http://deeplearning.hatenablog.com/entry/transformer",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539517042,
      "updated": 1539611316
    },
    {
      "id": "5bc403b4aff09e0000320f6b",
      "text": "\t[注意機構]",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539572660,
      "updated": 1539607508
    },
    {
      "id": "5bc32abeaff09e000044c68f",
      "text": " \t[注意機構は辞書オブジェクト]",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539517118,
      "updated": 1539609459
    },
    {
      "id": "5bc32a71aff09e000044c68c",
      "text": " \t[加法注意]と[内積注意]",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539517042,
      "updated": 1539572659
    },
    {
      "id": "5bc32b1baff09e000044c693",
      "text": "  [ソースターゲット注意]と[自己注意]",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539517212,
      "updated": 1539572659
    },
    {
      "id": "5bc32b21aff09e000044c694",
      "text": "  [縮小付き内積注意] (Scaled Dot-Product Attention) ",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539517217,
      "updated": 1539572659
    },
    {
      "id": "5bc48528aff09e000004d8c5",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605800,
      "updated": 1539605800
    },
    {
      "id": "5bc48528aff09e000004d8c6",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605801,
      "updated": 1539609344
    }
  ]
}