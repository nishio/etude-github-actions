{
  "id": "58ff85c7d84913001174f69e",
  "title": "「k-NNの前にPCAを掛けると精度が上がる」について",
  "created": 1493140938,
  "updated": 1493142461,
  "lines": [
    {
      "id": "58ff85c7d84913001174f69e",
      "text": "「k-NNの前にPCAを掛けると精度が上がる」について",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493140938,
      "updated": 1493142461
    },
    {
      "id": "58ff85dcaff09e0000c77913",
      "text": "\t「k-NNの前にPCAで次元削減をすると精度が上がる」→NG",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493140956,
      "updated": 1493141279
    },
    {
      "id": "58ff860eaff09e0000c7795c",
      "text": "\t「PCAで次元を削減すると計算に掛かる時間が減る」→OK",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493141007,
      "updated": 1493141281
    },
    {
      "id": "58ff8af3aff09e0000c7796e",
      "text": " 「教師データを増やせば精度が上がる」→OK",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493142259,
      "updated": 1493142297
    },
    {
      "id": "58ff8647aff09e0000c7795d",
      "text": "\t「元データの相関が高い場合に、PCAで次元削減をしても、精度は下がりにくい」→OK",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493141063,
      "updated": 1493141282
    },
    {
      "id": "58ff8680aff09e0000c7795f",
      "text": "\t\t「元データの相関が高い場合に、PCAで次元削減をしても、精度は下がらない」→NG",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493141121,
      "updated": 1493141290
    },
    {
      "id": "58ff8689aff09e0000c77961",
      "text": "\t\t「元データの相関が高い場合に、PCAで次元削減をすると、精度が上がる」→NG",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493141129,
      "updated": 1493141291
    },
    {
      "id": "58ff8b3baff09e0000c77970",
      "text": "\t上記の合算により「元データの相関が高い場合、k-NNの前にPCAで次元削減をしても精度は下がりにくく、計算にかかる時間は減るので、その分教師データを増やせば精度が上がる」→OK",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493142331,
      "updated": 1493142454
    },
    {
      "id": "58ff8b39aff09e0000c7796f",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493142329,
      "updated": 1493142331
    },
    {
      "id": "58ff8685aff09e0000c77960",
      "text": "\t「PCAは回転なので、次元削減しないならk-NNの精度は変わらない」→OK",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493141125,
      "updated": 1493141451
    },
    {
      "id": "58ff87cbaff09e0000c77963",
      "text": "\t「ある軸が原因でオーバーフィットしている時に、その軸を切り落とせば、汎化性能が上がる」→OK",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493141451,
      "updated": 1493141531
    },
    {
      "id": "58ff880daff09e0000c77964",
      "text": " \t「PCAの『分散が少ない軸』が、オーバーフィットの原因である」→その確率はゼロではないが、他の軸に比べて低い",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493141518,
      "updated": 1493141701
    },
    {
      "id": "58ff886faff09e0000c77966",
      "text": "  PCAで捨てられる軸は「教師データのラベル情報yをまるっと無視した上で、xの分散が小さい方向」であり、「識別する上で重要度が低い」とか「識別する上でノイズになる」とかではなく、単に「xがその方向にはあまり動かない」にすぎないから。",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493141616,
      "updated": 1493141717
    },
    {
      "id": "58ff88d5aff09e0000c77968",
      "text": "\t「k-NNは元データのスケールの影響を大きく受ける」→OK",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493141717,
      "updated": 1493141740
    },
    {
      "id": "58ff88ebaff09e0000c7796a",
      "text": " \t「なので正規化すると精度が上がる」→NG",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493141740,
      "updated": 1493141756
    },
    {
      "id": "58ff88fbaff09e0000c7796b",
      "text": "  \t正規化とは「各軸の分散が1になるように調整すること」で、それは「すべての軸が均等に重要であるにも関わらず、センサの値域の都合などで分散に差が出ている場合に、分散の小さい軸が不利にならないようにする」ということ。",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493141756,
      "updated": 1493141920
    },
    {
      "id": "58ff89aeaff09e0000c7796c",
      "text": "\t\t\tPCAは「軸x1と軸x2は相関が強いから、x1 + x2の分散は大きく、x1 - x2の分散は小さい。これを新しい軸にしよう」という軸回転の手法で、回転の後「で、分散の小さいx1 - x2の軸は無視しよう」と次元削減が組み合わされることが多い。分散の小さい軸の一部を無視して、残りを拡大するのは、割りと変な行為。",
      "userId": "582e63d27c56960011aff09e",
      "created": 1493141935,
      "updated": 1493142230
    }
  ]
}