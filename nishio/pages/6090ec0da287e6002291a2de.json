{
  "id": "6090ec0da287e6002291a2de",
  "title": "Hatena2014-02-12",
  "created": 1392130800,
  "updated": 1392130800,
  "lines": [
    {
      "id": "6090ec30aff09e00003dee82",
      "text": "Hatena2014-02-12",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee83",
      "text": "code:hatena",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee84",
      "text": " <body>",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee85",
      "text": " *1392179496*論文紹介「階層 Pitman-Yor 過程に基づく可変長 n-gram 言語モデル」",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee86",
      "text": " 既存の方法だと8-gramの言語モデルとか現実的に無理なんだけども、この方法だと可変長だから必要なところだけ長いgramにできるよ、という論文。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee87",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee88",
      "text": " Pitman-Yor過程と等価な中華料理店過程でツリーを作る。she willの後にsingが来たとするとルートノードからwillをたどってsheをたどったところにsingに対応する客を追加する。上位のノードにも代理客を追加する。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee89",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee8a",
      "text": " 既存手法(HPYLM)だと、客を追加する高さが固定。しかし決まった高さに追加するのではなく「客がルートから進んできて、テーブルを通過する確率」を各ノードごとに考える。この確率qiは未知、どうやって推定するか？",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee8b",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee8c",
      "text": " qiの事前分布がベータ分布であるとし、まず配置する。そこからランダムな単語wtを取り除いて、残りの客の分布からベータ分布のパラメータを更新、そのベータ分布からサンプリングして新しい深さに客を追加。この「取って、入れる」を繰り返すことで正しい分布に収束する（ギブスサンプリング）",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee8d",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee8e",
      "text": " サンプリングの際に深さの最大値を決めておくか、確率が十分小さくなるまでやるか、の2通りがある。後者の場合は完全にn-gramのnに相当するパラメータが消えてなくなるので∞-gramと呼べるだろう。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee8f",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee90",
      "text": " 実装にはスプレー木を使う。各ノードは(intもptrも64bitとして)64バイトのサイズがある。10000Kノードでmax 8-gramの言語モデルを作ることができているので、つまり640MBくらい。現実的サイズ。既存手法では30000種類の語彙の8乗のオーダーで記憶領域が必要なので現実的ではないが、提案手法では必要なところだけ長いgram数を使うので実現できている。実際5-gramの文脈と推定される箇所は3-gramと推定される箇所の半分くらいで、3-gramの記憶の9億倍を5-gramにつぎ込む既存手法は合理的でない。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee91",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee92",
      "text": " 既存手法で可能な3-gram、5-gramについては若干既存手法よりパープレキシティが高いが、英語コーパスに関しては7-gram，8-gramは既存手法5-gramより低い。日本語コーパスは…低くない。∞-gramも低くない。このへんは若干残念な感じだけども、とにかくノード数の削減効果が大きい！というのが長所だろうか。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee93",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee94",
      "text": " 感想: Googleさん、max 8-gramで作った言語モデルをください",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee95",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee96",
      "text": " 追記: 実装してみてわからなかったところを追記。「thwはwがp(w|h)からではなく、親ノードp(w|h')から生成されたと推定された回数」このsum_wであるthを保存するフィールドがノードの定義にあるけど、これはどのタイミングでどんな値を入れたらいいのか。またthしか保存されてないけどthwはどうやって復元するのか。dとθの推定方法は書いていなかったけども元のHPYLMの論文の最後におまけみたいに付いているdmとθmのサンプリングの式を使って推定するのか？",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee97",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee98",
      "text": " ** 実装",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee99",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee9a",
      "text": " https://github.com/nishio/VPYLM/blob/master/t.py",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee9b",
      "text": " 注: 新しいテーブルが作られる確率を無視しているのでまだ正しく動きません。通して動かすことはできるので雰囲気を知るぐらいで。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee9c",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee9d",
      "text": " ** 出典",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee9e",
      "text": " http://chasen.org/~daiti-m/paper/ipsj07vpylm.pdf",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee9f",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deea0",
      "text": " 次に",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deea1",
      "text": " ** さいごに",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deea2",
      "text": " この記事はDeep Learningに興味を持った著者が、関連論文を読んで勉強しながら書いているものです。そのため間違いなどが含まれている可能性があります。もしなにか気になる点がありましたらご指摘いただけるとありがたいです。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deea3",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deea4",
      "text": " *1392213182*論文紹介「ガウス過程に基づく連続空間トピックモデル」",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deea5",
      "text": " Deep Learningで単語を連続なユークリッド空間に投影したらそれが意味を表現すると盛り上がっているけど、それをやるのに最適化の難しいニューラルネットを使わないで正規分布だと思って最適化したらいい、という論文。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deea6",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deea7",
      "text": " word2vecには直接論文上は言及していない。制限ボルツマンマシン(RBM)は一般にLDA(Letent Dirichlet Allocation)を超えるって言われてて、更に多層にして\"Deep Learning\"したら更に性能が上がるよ、なんて報告されているけど、最適化が難しいし、何を学習してんのかわかんないし、他のモデルとの接続が難しい。だから研究の場ぐらいでしか使われてない。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deea8",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deea9",
      "text": " RBMでは隠れ層への重みが意味をコードしていて、LDAではp(k|w)が意味をコードしていて、結局のところ「単語がd次元連続ベクトルに対応している」というコンセプトが重要。というわけで単語をd次元ベクトルに対応づける未知の関数φがあって、このd次元ベクトルφ(w)がφ(w) ~ N(0, I_d) と仮定しよう。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeaa",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeab",
      "text": " 単語の出現確率はデフォルトの確率（これは最尤推定で求める）にexp(f(w))を掛けたものとする。fはガウス過程に従う曲面。ガウス過程の意味はまだよくわかってない。要するに次元を固定していない正規分布のようなものというイメージ。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeac",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deead",
      "text": " fは計算が難しいのでやっぱりd次元の正規分布に従うuを便宜的に導入して、f = Φuとする。uは文書と1対1対応。要するに文書にも単語と同じ空間上に位置を与えて、どの単語が出やすくなるかという情報を内積で表現するということね。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeae",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeaf",
      "text": " 実際の単語の出現頻度の分布はガウス分布っぽい時もあるけど少し頻度高い側に裾が長い分布になっているのでa0をパラメータとするPolya分布を使うことにする。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeb0",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeb1",
      "text": " 潜在変数φ(w), u, a0を観測された単語の出現頻度からいい感じに求めたい。そこでマルコフ連鎖モンテカルロ(MCMC)の一つメトロポリスヘイスティング(MH)を使う。要するに一つの変数以外を止めて、選んだ変数を元の値からマルコフ的に決めて、新しい値がMH基準で受理されればそれを使って更新、ダメだったら元の値に戻す、っていう遺伝的アルゴリズム風のことをやる。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeb2",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeb3",
      "text": " まとめ：提案手法CSTMはRBMを連続なガウス分布にしたものともみなせて、MHで簡単に最適化でき、正規化は事前分布を置いてベイズ生成モデルにしていることで自然に正規化される。混合モデルではなく積モデルであることが原因で次元数が大きすぎるときに問題が起きるので次元数自体をベイズ的に行うことが今後の課題(CRPみたいに？？)",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeb4",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeb5",
      "text": " 感想：今回、文書ごとにuがあるってモデルなので、直接的にword2vecと同じようなことはできないという理解。「各単語の前後何単語」ってウィンドウで切って、それぞれのウィンドウを文書であるとみなせばいいのかなぁ。まあでもニューラルネットをブラックボックスとして扱うよりは何をやっているか少しわかりやすいのかもしれない。word2vecの論文をまだしっかり読んでないので、それを読んでから考える。あと共著に後藤真孝先生が入っているなぁと思ったら謝辞にOngaCRESTプロジェクトって書いてあったから、自然言語だけじゃなくて音楽への応用も視野に入っているんだろうなぁ。それはそれで面白そう。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeb6",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeb7",
      "text": " ** 出典",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeb8",
      "text": " 論文: http://chasen.org/~daiti-m/paper/nl213cstm.pdf",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeb9",
      "text": " ガウス過程の解説だけどさっぱりわからない: http://www.ism.ac.jp/~daichi/paper/svm2009advgp.pdf",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deeba",
      "text": " MCMCとMHの解説: http://ebsa.ism.ac.jp/ebooks/sp/sites/default/files/ebook/1881/pdf/vol3_ch10.pdf",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deebb",
      "text": " ",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deebc",
      "text": " ** さいごに",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deebd",
      "text": " この記事はDeep Learningに興味を持った著者が、関連論文を読んで勉強しながら書いているものです。そのため間違いなどが含まれている可能性があります。もしなにか気になる点がありましたらご指摘いただけるとありがたいです。",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deebe",
      "text": " </body>",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deebf",
      "text": "",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deec0",
      "text": "[はてなダイアリー 2014-02-12 https://nishiohirokazu.hatenadiary.org/archive/2014/02/12]",
      "created": 1392130800,
      "updated": 1392130800,
      "userId": "582e63d27c56960011aff09e"
    }
  ]
}