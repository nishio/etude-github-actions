{
  "id": "6419cd14d78b0e001b9891fb",
  "title": "性能向上が頭打ちになるか、際限なく性能が向上するか",
  "created": 1679412501,
  "updated": 1679421945,
  "lines": [
    {
      "id": "6419cd14d78b0e001b9891fb",
      "text": "性能向上が頭打ちになるか、際限なく性能が向上するか",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412501,
      "updated": 1679412652
    },
    {
      "id": "6419cd87aff09e000091a8c6",
      "text": "「人類がこれまで言語その他の情報の形で書き溜めた知識の総体」を学習し切ったところで性能向上が頭打ちになるか、際限なく性能が向上するか",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412615,
      "updated": 1679412643
    },
    {
      "id": "6419cd87aff09e000091a8c7",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412615,
      "updated": 1679412615
    },
    {
      "id": "6419cd43aff09e000091a89f",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637758056342880256] GPTでAI界隈が沸騰している。開発者も含めて誰も急激な性能向上の理由を理解出来ていない。普段は半年や1年で古くなるような時事ネタはあまり呟かないことにしているが、このところの動きがあまりに早く、未来に向けての不確実性が高まっているので、少し現時点でのシナリオ整理をしたい。(1/15)",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8a0",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8a1",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637758122210230278] まず、現状を整理する。最近の成果はそのほとんどがトランスフォーマーと呼ばれるエンコーダ・デコーダモデルによる。注目すべきはこれが畳み込みや再帰といった並列計算を防げる仕組みを廃したために計算力の集約が可能になり、飛躍的に大規模なデータセットでの学習が可能になった事だ。(2/15)",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679421509
    },
    {
      "id": "6419cd43aff09e000091a8a2",
      "text": "　[Transformer]",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679421518
    },
    {
      "id": "6419f04faff09e00005ddbac",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679421519,
      "updated": 1679421519
    },
    {
      "id": "6419cd43aff09e000091a8a3",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637758202300473344] そこで起きたことが、[スケーリング則の発見]だ(2020年)。  (https://arxiv.org/abs/2001.08361)     つまり、計算量、データサイズ、モデルの規模の３つを同時に大きくしてゆくことで、あたかも上限なくモデルの性能が上がってゆくように見える現象だ。(3/15)arxiv.org",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679421531
    },
    {
      "id": "6419cd43aff09e000091a8a4",
      "text": "> Scaling Laws for Neural Language Models",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8a5",
      "text": "> We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training,...",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8a7",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8a8",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637758313478881280] さらに2022年になって、10の23乗から24乗回あたりの計算量を境に急激に性能が向上するという現象が確認された。ある程度予測可能なスケーリング則から非連続的なテイクオフに移行したように見えるため、今後何が起きるのかが見えにくくなっている(https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html…)。　(4/15)",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8a9",
      "text": "> [https://pbs.twimg.com/card_img/1635793054769754113/tkB1TptV?format=png&name=900x900#.png]",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8aa",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8ab",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637758476930940928] そこで一旦基本に戻る。機械学習モデルが出来るのは学習に使ったデータからの帰納だ（既に見たことがあることしか予測出来ない）。しかしGPT3/4は柔軟な応答や多段論法など一見学習データセットから直接的に導けるとは思えない演繹的なタスクを実行しているように見える。可能な説明は二つある。(5/15)",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8ac",
      "text": "　ここを帰納とか演繹とかに分類しようとする人が多いのだけど、その二つを背反なものだと考えることが[誤った二分法]の可能性がある",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679421693
    },
    {
      "id": "6419f0fdaff09e00005ddbad",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679421693,
      "updated": 1679421693
    },
    {
      "id": "6419cd43aff09e000091a8ad",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637758545738493953] １つ目は我々がこれまで演繹と思っていたものの大部分が帰納だったという可能性だ。例えばシマウマと聞いて縞模様のあるウマを想起するとき、ある特徴とあるモノとを組み合わせて別のモノを導き出すこれと同型のパターンはデータセットのどこかに含まれていた。(6/15)",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8ae",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8af",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637758636218028032] おそらく10の24乗FLOPSというのは人類が言語情報の形で蓄積した知識の総体から[意味ネットワーク]を抽出するのに必要な計算量なのだろう。丁度その辺りの閾値を超え急激に意味ネットワークがつながり性能が向上した。この場合今後はシグモイド的（急激な上昇の後に停滞期が来る）に推移するだろう。(7/15",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679421718
    },
    {
      "id": "6419cd43aff09e000091a8b0",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8b1",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637758697056403456] ２つ目の可能性は、北川さん(@takuyakitagawa)やgoogleのブログにあるように、ネットワークモデルに創発的（相転移的）な現象が起きているということだ。つまり、計算力の適用によりデータセットには明示的に含まれていない新しい連関や意味ネットワークが生まれているという可能性だ。(8/15)",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412593
    },
    {
      "id": "6419cd43aff09e000091a8b4",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8b5",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637758837032906752] 数学で公理系から様々な定理や命題が生み出されるように、言語データに含まれる情報から新しい情報が生み出される。人類の頭脳がその一部しか探索してこなかったなら今後AIがもっと深くて広い知的探索を担うかもしれないシナリオだ。言語システム自体が演繹性を持つ可能性とも言える。(9/15)",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8b6",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8b7",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637758908919091202] これらのどちらなのかは、あと数ヶ月から１、２年くらいで明らかになるかもしれない。(10/15)",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8b8",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8b9",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637758974140497922] もし１つ目の可能性が正しい場合、計算量とモデル規模の伸びに対していずれ学習データ量が追従出来なくなり、「人類がこれまで言語その他の情報の形で書き溜めた知識の総体」を学習し切ったところで性能向上は頭打ちになるだろう。(11/15)",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8ba",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8bb",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637759088783409152] ２つ目の可能性が正しい場合には、当面は際限なく性能が向上するように見えるだろう。その場合、計算力に関する物理的な制約がクリティカルになることは何度か紹介している私の2018年の論文でシナリオ整理している通り( https://jstage.jst.go.jp/article/jjsai/33/6/33_867/_article/-char/ja/… )。(12/15)jstage.jst.go.jp",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8bc",
      "text": "> レクチャーシリーズ：「シンギュラリティとAI」〔第7 回〕将来の機械知性に関するシナリオと分岐点",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8bd",
      "text": "> 人工知能, 2018 年 33 巻 6 号 p. 867-872",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8bf",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8c0",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637759157146365952] 現在の言語モデルベースのAIは能動性や身体性が欠けている点で限界があるが、機械学習モデルにツールやセンサーを使いこなさせるための仕組み（認知アーキテキチャ）の研究は様々なところで取り組まれている。(13/15)",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419f1e6aff09e00005ddbae",
      "text": "　[能動性]",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679421926,
      "updated": 1679421945
    },
    {
      "id": "6419cd43aff09e000091a8c1",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd43aff09e000091a8c2",
      "text": ">[ktakahashi74 https://twitter.com/ktakahashi74/status/1637759270791049216] ロボットやネットツールなどを使って能動学習を行うAIの開発に根本的な技術上の壁はないので、そうなれば理論上は「人類のこれまでの知識の総体」を上限とする理由が無くなり、物理現象の時定数のみが制限として残る(上記論文参照)。このあたりがさらに先を見たシナリオ分岐に関係するだろう。(14/15)",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679421917
    },
    {
      "id": "6419cd43aff09e000091a8c4",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412547,
      "updated": 1679412547
    },
    {
      "id": "6419cd56aff09e000091a8c5",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412567,
      "updated": 1679412567
    },
    {
      "id": "6419cd29aff09e000091a89e",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1679412522,
      "updated": 1679412522
    }
  ]
}