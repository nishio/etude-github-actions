{
  "id": "5bc49d72acf0090013445d12",
  "title": "ハード注意機構",
  "created": 1539612020,
  "updated": 1539684682,
  "lines": [
    {
      "id": "5bc49d72acf0090013445d12",
      "text": "ハード注意機構",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539612020,
      "updated": 1539612020
    },
    {
      "id": "5bc49d7aaff09e000058cfd0",
      "text": "注意機構",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539612026,
      "updated": 1539612029
    },
    {
      "id": "5bc49d74aff09e000058cfcf",
      "text": "[$ Attention(query, Key, Value) = Normalize(Func(query, Key)) \\cdot Value]",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539612020,
      "updated": 1539612022
    },
    {
      "id": "5bc49d74aff09e000058cfce",
      "text": "のNormalizeの部分がSoftmaxではなく「Softmaxの結果からサンプリングしてone-hot表現を返す関数」になってるもの。",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539612020,
      "updated": 1539612094
    },
    {
      "id": "5bc49dfdaff09e000058cfd4",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539612157,
      "updated": 1539612157
    },
    {
      "id": "5bc49dc6aff09e000058cfd2",
      "text": "\t微分できなくなるというとても大きなデメリットがあるが、どういうメリットがあるのだろう？",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539612102,
      "updated": 1539684391
    },
    {
      "id": "5bc5b827aff09e00007777fc",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539684392,
      "updated": 1539684392
    },
    {
      "id": "5bc5b828aff09e00007777fd",
      "text": "\tValuesの空間がベクトル空間でない場合に応用できる",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539684392,
      "updated": 1539684415
    },
    {
      "id": "5bc5b83baff09e00007777fe",
      "text": " \tソフト注意をするためにはValuesの空間はスカラー倍と加算ができなければならない",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539684411,
      "updated": 1539684432
    },
    {
      "id": "5bc5b84faff09e00007777ff",
      "text": "  例えばシンボルの空間はベクトル空間ではない",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539684432,
      "updated": 1539684443
    },
    {
      "id": "5bc5b870aff09e0000777800",
      "text": "  「Softmaxの結果からサンプリングしてone-hot表現を返す関数」",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539684464,
      "updated": 1539684471
    },
    {
      "id": "5bc5b877aff09e0000777801",
      "text": "\t  one-hot表現xについて[$ x\\cdot V = V[x] ]",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539684471,
      "updated": 1539684536
    },
    {
      "id": "5bc49e97aff09e000058cfd6",
      "text": "\t\t単語を出力する系のNNで|V|のベクトルからサンプリングしてるのハード注意機構だと解釈できる",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539612311,
      "updated": 1539684584
    },
    {
      "id": "5bc5b8e7aff09e0000777803",
      "text": "  \t単語がValueで単語のone-hot表現がKey",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539684584,
      "updated": 1539684607
    },
    {
      "id": "5bc5b907aff09e0000777804",
      "text": "   単語が分散表現になっている場合はもっとわかりやすい。内積とってSoftmaxしてそこからサンプリング=ハード注意",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539684615,
      "updated": 1539684682
    },
    {
      "id": "5bc5b8bfaff09e0000777802",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539684543,
      "updated": 1539684544
    },
    {
      "id": "5bc49e97aff09e000058cfd5",
      "text": "https://jhui.github.io/2017/03/15/Soft-and-hard-attention/",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539612311,
      "updated": 1539612312
    },
    {
      "id": "5bc49e98aff09e000058cfd7",
      "text": ">Hard attention replaces a deterministic method with a stochastic sampling model. To calculate the gradient descent correctly in the backpropagation, we perform samplings and average our results using the Monte Carlo method. Monte Carlo performs end-to-end episodes to compute an average for all sampling results. The accuracy is subject to how many samplings are performed and how well it is sampled. On the other hand, soft attention follows the regular and easier backpropagation method to compute the gradient. However, the accuracy is subject to the assumption that the weighted average is a good representation for the area of attention. Both have their shortcomings. Currently, soft attention is more popular.",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539612312,
      "updated": 1539612339
    },
    {
      "id": "5bc49dcfaff09e000058cfd3",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539612112,
      "updated": 1539612112
    }
  ]
}