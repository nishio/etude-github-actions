{
  "id": "5bc481f4f2c031001339745f",
  "title": "自己注意",
  "created": 1539604987,
  "updated": 1539611855,
  "lines": [
    {
      "id": "5bc481f4f2c031001339745f",
      "text": "自己注意",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539604987,
      "updated": 1539604987
    },
    {
      "id": "5bc481fdaff09e000058cfc2",
      "text": "僕はかつて[注意機構]について、RNNをイメージして「過去の隠れ状態を取っておいて、それを元に注意強度を計算し…」と考えていたが、注意機構だけで成果を出した[Transformer]はRNNではないので、RNNを前提にしたメンタルモデルは間違い。",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539604989,
      "updated": 1539611855
    },
    {
      "id": "5bc48414aff09e000004d8bf",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605524,
      "updated": 1539605524
    },
    {
      "id": "5bc48413aff09e000004d8be",
      "text": "RNNを暗黙に想定していると「ソースターゲット注意」ではない「自己注意」の概念がピンとこなくなるので、一旦忘れる必要がある。",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605523,
      "updated": 1539605549
    },
    {
      "id": "5bc48499aff09e000004d8c1",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605657,
      "updated": 1539605657
    },
    {
      "id": "5bc482e2aff09e000004d8b3",
      "text": "自己注意は自分自身をソースとターゲットにする。",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605218,
      "updated": 1539605662
    },
    {
      "id": "5bc4849eaff09e000004d8c2",
      "text": "\tTransformerのデコーダで使われている注意は自己注意は入力源がデコーダの下層レイヤから入力を受け取る",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605662,
      "updated": 1539605663
    },
    {
      "id": "5bc48316aff09e000004d8b6",
      "text": " \tこの場合、RNNよりはむしろ[CNN]の方が似ている",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605270,
      "updated": 1539605609
    },
    {
      "id": "5bc483afaff09e000004d8b7",
      "text": "  一切リカレントしないので「過去の隠れ層の値を取っておいて…」という古い「注意機構」の説明が頭に残ってると混乱するから忘れる必要がある",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605424,
      "updated": 1539605672
    },
    {
      "id": "5bc483f1aff09e000004d8bc",
      "text": "\t\t自分の下のレイヤーからの入力に対して注意機構を使うので、どちらかというとCNNの発展だと考えた方が素直",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605490,
      "updated": 1539605697
    },
    {
      "id": "5bc483f1aff09e000004d8bd",
      "text": "\t\t画像由来のCNNは「ある点に対する影響は周囲の点が強いだろう」「点の周囲の点からの影響は一定だろう」という想定で周囲の点から固定の重みによる畳み込みをする",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605490,
      "updated": 1539605725
    },
    {
      "id": "5bc483f0aff09e000004d8b8",
      "text": "\t\tそしてその畳み込みサイズは固定なわけだが自己注意機構はその対象を入力全体に広げる。不定長の入力になる。",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605489,
      "updated": 1539605739
    },
    {
      "id": "5bc484eaaff09e000004d8c3",
      "text": "\t\tなぜそれが可能かというと注意機構の一般的な仕組みとして、注意重みが入力の関数で決まる仕組みだから。固定の重み行列を持つCNNなどと違って入力の個数は任意で良い。",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605739,
      "updated": 1539605748
    },
    {
      "id": "5bc484eaaff09e000004d8c4",
      "text": "",
      "userId": "582e63d27c56960011aff09e",
      "created": 1539605739,
      "updated": 1539605751
    }
  ]
}