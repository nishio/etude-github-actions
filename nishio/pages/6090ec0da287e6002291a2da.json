{
  "id": "6090ec0da287e6002291a2da",
  "title": "Hatena2014-02-08",
  "created": 1391785200,
  "updated": 1391785200,
  "lines": [
    {
      "id": "6090ec30aff09e00003dedd6",
      "text": "Hatena2014-02-08",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedd7",
      "text": "code:hatena",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedd8",
      "text": " <body>",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedd9",
      "text": " *1391792338*Deep Learning論文紹介「Generating Text with Recurrent Neural Networks」",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedda",
      "text": " リカレントニューラルネット(RNN)を使って文章を生成する話。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deddb",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deddc",
      "text": " RNNはパワフルだけど学習が難しい。Hessian-free最適化(HF)を使えばいい感じに学習できて、難しい問題に使える。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deddd",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedde",
      "text": " この論文では文字を入力として言語モデルを作って、そこから文章を生成する。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deddf",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dede0",
      "text": " 標準的なRNNだとちょっと問題があったのでmultiplicativeな亜種を作った。入力文字によって隠れ層のベクトルを次の層へ伝達する際の遷移行列を選べるようにしたものである。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dede1",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dede2",
      "text": " multiplicative RNN(MRNN)をHFで、8つのハイエンドGPUを積んだマシンで5日掛けて学習した結果、既存の言語モデルを上回る成績を得た。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dede3",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dede4",
      "text": " 隠れ層の活性化関数はtanh、出力層は素通し。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dede5",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dede6",
      "text": " Sequence memoizerはデータ構造の関係から32GBメモリのマシンで扱えるデータセットは130MBぐらい、MRNNはこの上限がない。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dede7",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dede8",
      "text": " 実験では86種類の文字から構成される100MBのデータを使った。隠れ層が1500個、factorが1500個で、490万個のパラメータがある。重みはスパースに初期化して、BPTTをアンロールすると500層の隠れ層があることに相当する。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dede9",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedea",
      "text": " (ちなみに僕の手元にある岩波文庫1冊分のスキャンデータは530KBなので100MBのデータはだいたい200冊分)",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedeb",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedec",
      "text": " 実験結果 memorizerに比べてbit per characterが小さい。文章を生成させたらちゃんと意味のわかる単語が生成される。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003deded",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedee",
      "text": " 感想 N-gramモデルで文章生成して遊んでみたことがあると、これを使ってどれくらい良くなったか試してみたくなる。文字を入力としているのに単語が作られていて興味深いが「どの範囲が単語か」を判定するにはこれだけではできないな。どうすればいいかな。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedef",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedf0",
      "text": " ** 出典",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedf1",
      "text": " http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedf2",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedf3",
      "text": " ** さいごに",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedf4",
      "text": " この記事はDeep Learningに興味を持った著者が、関連論文を読んで勉強しながら書いているものです。そのため間違いなどが含まれている可能性があります。もしなにか気になる点がありましたらご指摘いただけるとありがたいです。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedf5",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedf6",
      "text": " *1391825340*Deep Learning論文紹介「Learning Recurrent Neural Networks with Hessian-Free Optimization」",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedf7",
      "text": " リカレントニューラルネット(RNN)に長距離相関を学習させるのは難しい問題だったが、Hessian-Freeを使ったらできた、という話。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedf8",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedf9",
      "text": " RNNはBack Propagation Through Time(BPTT)＋確率的勾配法で簡単に計算できることが長所とされているが、10タイムステップほど離れた相関は1次の勾配法では全然学習できない",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedfa",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedfb",
      "text": " その原因は“vanishing/exploding gradients”。長距離相関はBPTTで何度も前の時刻の層へ伝搬されるため、誤差信号がすぐに減衰して消えてしまう。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedfc",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedfd",
      "text": " 最近生まれたHessian-Free(HF) またの名をtruncated-Newton, Newton-CG はDeep Neural Networksの学習に有効なので、RNNの学習にもきっと有効に違いない。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedfe",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dedff",
      "text": " 数式に関してはあんまり簡潔に要約できないので割愛。ここをじっくり読まないとこの論文は意味が無いなぁ。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee00",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee01",
      "text": " ざっくり言うと「2階微分を使って目的関数を近似してそれを最適化する系のやりかたではヘシアン(2階微分の行列)の逆行列を計算する必要が出てきてとても計算コストが高いよね」ということらしいが、Hessian-Freeでどうやって回避するのかはよくわからなかった。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee02",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee03",
      "text": " \"Hessian-Free\"自体は同じ著者James Martensの\"Deep learning via Hessian-free optimization\"で説明されている。こちらを先に読むべきか。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee04",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee05",
      "text": " ** 出典",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee06",
      "text": " http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Martens_532.pdf",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee07",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee08",
      "text": " ** さいごに",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee09",
      "text": " この記事はDeep Learningに興味を持った著者が、関連論文を読んで勉強しながら書いているものです。そのため間違いなどが含まれている可能性があります。もしなにか気になる点がありましたらご指摘いただけるとありがたいです。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee0a",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee0b",
      "text": " *1391838220*Deep Learning論文紹介「Deep learning via Hessian-free optimization」",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee0c",
      "text": " \"Hessian-Free\"という新しい最適化手法をDeep Learningのauto-encoderの学習に使ってみたら事前学習なしで既存の報告の性能を超えたぞ凄いだろう、という話。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee0d",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee0e",
      "text": " ニューラルネットのパラメータ決定はよく研究されている問題で、勾配法で効率よく計算できると言われている。しかしDeep Learningのように隠れ層がとても多いケースではうまくいかない。学習にとても時間がかかったり、学習データに対してさえ酷いパフォーマンスしか出せなかったりする(under-fitting)。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee0f",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee10",
      "text": " 最適化に関する研究者の間では勾配法が病的な曲率を持った目的関数に対しては不安定であることがよく知られている。2次の最適化法はこのような目的関数に対してもうまく働く。だからDeep Learningにもこの種の最適化を使ったらいいんじゃないか。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee11",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee12",
      "text": " でもまだいくつか問題がある。まずでかいデータセットに対して現実的な速度で動くようにすること。これは確率的勾配降下法(SGD)みたいな感じでオンライン学習にしたらいい。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee13",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee14",
      "text": " レイヤーごとにpre-trainingするってのも大きな進歩の一つだ。これをやってからSGDすると問題を回避できるようにみえる。実際、成功している応用例も色々ある。しかし疑問が残る：なんでこれで上手くいくの？なんで必要なの？ローカルミニマムが大量にあることが原因だという説明をしている研究者もいる。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee15",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee16",
      "text": " 別の説明として、目的関数が病的な曲率（例えば細長い谷とか）を持っていて、勾配法のような曲率を見ない最適化法では最適化不可能だ、というものがある。そこでこの視点に立って、セミオンラインの2次の最適化手法を提案する。これを使えばunder-fittingしないし既存のpre-trainingを使った方法より効率的。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee17",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee18",
      "text": " まず標準的な2次の最適化手法であるニュートン法について解説する。ニュートン法はパラメータの個数の2乗のサイズのヘシアンを計算する必要があり、でかいモデルの計算には現実的ではない。が、これを学ぶことで、もっと現実的な亜種(quasi-Newton methodsとは)がどう振る舞うのかについて理解できる。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee19",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee1a",
      "text": " ニュートン法は勾配法同様にパラメータを逐次的に更新していく手法。コアのアイデアは「目的関数fを二次関数で近似」(数式1) Bはヘシアンだと思っていいんだけど、正定値でないことがたまにあって最小値を持たなくて困るので適当に対角成分を増やして正定値にする(dump)",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee1b",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee1c",
      "text": " ニュートン法は\"scale invariance\"が重要な特徴。線形な変換では振る舞いが変わらない。これがないとパラメータのスケーリングが悪いと性能が悪化してしまう。またこの特徴によって学習率を調整したりとかしなくてよくなる。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee1d",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee1e",
      "text": " 逆に言えば現在のパラメータ周辺の曲率を元に暗黙のスケーリングが決まっていて、それによって勾配を変換しているという考え方ができる。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee1f",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee20",
      "text": " 曲率の絶対値が小さいなら勾配の変化は少ないので、長い距離進んでも良いってこと。逆に大きいなら、pの方向にセンシティブってことだからちょっとだけ進んで適切なpの方向を再確認すべき。（この段落、あとで数式を再確認する）",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee21",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee22",
      "text": " （まだ肝心の本題に入ってないけど続きはまた今度）",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee23",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee24",
      "text": " ** 出典",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee25",
      "text": " http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_Martens10.pdf",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee26",
      "text": " ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee27",
      "text": " ** さいごに",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee28",
      "text": " この記事はDeep Learningに興味を持った著者が、関連論文を読んで勉強しながら書いているものです。そのため間違いなどが含まれている可能性があります。もしなにか気になる点がありましたらご指摘いただけるとありがたいです。",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee29",
      "text": "  ",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee2a",
      "text": " </body>",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee2b",
      "text": "",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    },
    {
      "id": "6090ec30aff09e00003dee2c",
      "text": "[はてなダイアリー 2014-02-08 https://nishiohirokazu.hatenadiary.org/archive/2014/02/08]",
      "created": 1391785200,
      "updated": 1391785200,
      "userId": "582e63d27c56960011aff09e"
    }
  ]
}