Consider an eye-less thinking support system.
from [fear of being lost].

Think about [thought support systems] [that do not use the eyes].

[Writing and thinking" is a prerequisite for using the eyes].

I think advances in [voice input] have made it possible to do one-way output with a fair degree of accuracy.
	There are more misconversions, misspellings, etc. than when output is done with the eyes and hands.
	has to some extent achieved its goal of retaining memory in the main.
But there's still a problem with running the feedback loop of thinking about what you've written while looking at what you've written.
Why is that?
	Maybe it's because I can't read what I write in Scrapbox.
	→Read out [Scrapbox is difficult to use if you are blind].


For chat services such as Keicho used on smartphones, regardless of whether the user is blind or not, there are natural use cases where the user can use the service while taking a walk, etc. using only the ear and microphone without looking at the screen, so the ability to use the service without using visual feedback is considered useful to a certain extent.

The current Keicho system is inconvenient to use with voice only because it does not read it back
　It would be nice if a notepad that allows you to jot down your thoughts with just your ears and voice while taking a walk could be realized first as a problem before the chatbot functionality.
　What is needed in that case
　Voice input and the input is first immediately repeated back to the user.
　[VoiceOver] allows you to
　Correct or restate mistakes to create a state in which everything you want to output is output.
　The trouble with this is that the voice recognition system may not output what we want it to output as we have described in our voice.
　For example, in this example, the word "Keicho," which is the six letters of the alphabet, is based on the image of "Keicho" in the book, but since this is a coined word, the speech recognition system cannot recognize it, and the word is input based on the lightness of the year.
　 I dare you to leave a misconversion.

　It seems to me that there needs to be more functionality to absorb shaky notation than there is for text input.

　Sometimes it's hard to get certain words across, even when you're dealing with humans.
　　Words that are difficult to input with voice input can be viewed as words that are difficult to convey to a virtual persona called a computer.
　　In that case, what is natural for humans to do?
　　It's natural to use a different concept, but there are times when you want someone to learn a word that expresses a particular concept.
　　I hope the notepad side remembers the words.

How can we do that?
For example, if the system can remember that in the context of this memo, a particular word (e.g. Keicho) refers to the chat system.
How can we do that?