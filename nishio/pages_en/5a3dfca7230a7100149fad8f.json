attention
attention (heed)
	addition caution
 internal volume caution
 Source Target Attention
 self-caution

	Note A is defined with query q, key k, and value v as follows
	[$ A(Q, K, V) = \mathrm{softmax}(QK^T)V]
 Additive and Intra-product Caution
  Theoretically, the complexity is about the same, but the inner product attention can be computed with matrix products, so it is faster in practical use.
  The claim that scaling with the dimension dk of the key gives better performance
	[$ A(Q, K, V) = \mathrm{softmax}({QK^T \over \sqrt{d_k}})V]
