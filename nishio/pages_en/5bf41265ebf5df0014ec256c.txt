Embedded vectors good or bad
What determines whether an [embedded vector] is good or bad?

For two words that are close together
	Identical: B to A (Python and Pyth0n) → Replace B in original data with A
 Synonyms: A and B mean the same thing (please see below) → Keep the original data, but unify the mapping between words and IDs.
	Synonym: [synonym] is → this is OK as it is
 Contrast: A and B are [synonyms] → add 1 axis for contraries to the vector and assign +1/-1 as appropriate
 Concatenation: A and B are a single meaning in the form "AB" → need to add vocabulary, be creative when reading input

Teaching conjunction increases vocabulary. Teaching the same decreases it.
This teacher data itself can be reused.

I need to make some modifications to the learning process, I want to use vectors around, and in the end I need to create my own [word2vec]-like system?

[good or bad dispersion representation].
[Distributed representation]
