Improved Techniques for Training GANs
Improved Techniques for Training [GAN]s
https://arxiv.org/pdf/1606.03498.pdf
[Salimans+ 2016]

Proposal for [Feature Matching
	[https://gyazo.com/d1d2456f5324365f5dc87b520ffecbb6]
	Feature matching addresses the instability of GANs by specifying a new objective for the generator that prevents it from overtraining on the current discriminator. 
	Instead of directly maximizing the output of the discriminator, the new objective requires the generator to generate data that matches the statistics of the real data, where we use the discriminator only to specify the statistics that we think are worth matching. 
 Specifically, we train the generator to match the expected value of the features on an intermediate layer of the discriminator. This is a natural choice of statistics for the generator to match, since by training the discriminator we ask it to find those features that are most discriminative of real data versus data generated by the current model.

Therefore, instead of increasing the correctness rate of the discriminator, the discriminator is trained so that the value of the middle layer of the discriminator when real data is inserted and the value of the middle layer of the discriminator when generated data is inserted are as far apart as possible.

Together with the [Minibatch discrimination] proposal
	Avoid Mode Collapsing
 Mode Collapsing is a common GAN problem: "I trained it with MNIST and it only produces 1's."
 If we ignore the noise source z and always generate 1, D won't think it's strange, so G will think it's easier that way.
 Therefore, the objective function should be a positive evaluation of the variability within Minibatch.


[Improved Techniques for Training GANs arXiv:1606.03498 - What do you order Machine Learning? http://musyoku.github.io/2016/12/23/Improved-Techniques-for-Training-GANs/]
