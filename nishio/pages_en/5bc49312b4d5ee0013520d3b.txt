positional encoding
	Representation of position by mixing a rotating unit vector as position information
 	Unlike CNN, the attention mechanism does not have position information, so it cannot realize "what is the previous word" by itself, but when the word is changed to [embedded vector], the position information of the word is also embedded, so the attention mechanism can make decisions based on the position. So the attention mechanism can make judgments based on position.
		It is also clever that this position is represented by a rotating unit vector, so that the relation "n words before" can be easily expressed by matrix multiplication when encoded in this way, and how much ambiguity is allowed can be expressed by inserting a number of different rotation cycles.
		This combination of [positional encoding] and [self-attention] is supposed to be great!
