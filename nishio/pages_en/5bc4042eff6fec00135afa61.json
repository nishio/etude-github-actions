注意機構
[注意]([Attention])
Generalization as of 2018
	[$ \mathrm{Attention}(query, Keys, Values) = \mathrm{Normalize}(F(query, Keys)) \cdot Values]
	There are queries and Keys, which are a bunch of multiple keys.
 There is a function F that takes a query and Keys as arguments and returns the intensity of attention for each key.
 The results are normalized in some way to sum to 1 to obtain the attention intensity (roughly softmax, see [hard attention mechanism]).
 Weighted average Values by their attention intensity

	図解
		[https://gyazo.com/211618e709ff284a379c5c2f502934da]

	F does not know the number of Key. [$ F(query, Key)] does not depend on the shape of Key.
	 I don't know how to express this in mathematical terms.
  一つのqueryと一つのkeyを受け取る関数fがあって`[f(query, key) for key in Keys]`

2014年 [加法注意] [1409.0473 Neural Machine Translation by Jointly Learning to Align and Translate https://arxiv.org/abs/1409.0473]
	Func := Feed-Forward Network
	[$ Attention(query, Key, Value) = Softmax(FFN(concat(query, Key))) \cdot Value]
 >By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector. With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.
 The hidden state of the RNN is a fixed-length vector, and it is a burden to remember to pack the entire sentence's data into it
 Attention mechanism can retrieve information from data of arbitrary length, thus reducing its burden
  [https://gyazo.com/dab69f04c581681e9c3c543b92633ef5]


2015年 [内積注意] [1508.04025 Effective Approaches to Attention-based Neural Machine Translation https://arxiv.org/abs/1508.04025]
	Split that query and key are simply inner products of query and key
	[$ Attention(query, Key, Value) = Softmax(query \cdot Key) \cdot Value]
	Of course, this inner product is sometimes expressed as a matrix product in some papers.
	関連 [双線形]
[https://gyazo.com/1db88d9a61dcce7af368f0d0e594b3f1]

[source target note] and [self note] ([2017 https://arxiv.org/abs/1703.03130])
	Initially, the attention mechanism was envisioned to be used in combination with RNNs
 Store the Encoder's hidden states in the Encoder-Decoder configuration and select from among those hidden states by the attention mechanism
 この構成だとKey, ValueはEncoderから、queryはDecoderから来る
 This type of configuration is called [source-target attention
 	([Sequence Generation with Target Attention http://ecmlpkdd2017.ijs.si/papers/paperID307.pdf](2017)でsource-target attentionとtarget-target attentionという形で比較議論されている)
 K and V together are called Memory
 対義語が[自己注意](Self-attention)
  This one is Key, Value, query all are self... no, that definition is not balanced by the level of abstraction...
  It may eventually differentiate into better terms.
  So far, one implementation example is "everything comes from the lower layers."
  	In this form, it's a development of [CNN].
   CNNs that could only accept fixed-length input were replaced by [attention mechanism] that can accept indefinite-length input.
			Commentary on this replacement: [CNN and self-attention].


-----
古い解説
	This commentary implicitly assumes [RNN] and is not generalized: it is not a generalization.
 	Save [hidden state] in the past.
  Create a scalar that represents the appropriate intensity of attention from "the current hiding state and its hidden states."
  Normalize that scalar to a total of 1
  Used for a weighted average of each hidden state
 　　There is a way to use output layer values instead of past hidden states.