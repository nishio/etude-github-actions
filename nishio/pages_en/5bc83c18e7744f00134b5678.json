Model compression by distillation
	Cannot be deployed with current hardware constraints
 However, hardware performance will continue to increase, so it will be possible to work with a mere [ensemble] in the near future.

	Learning student models with output of teacher models
 	Cases where Softmax output is used as-is instead of one-hot
		[Soft target loss]
 	Equivalent to [Label Smoothing

[1503.02531 Distilling the Knowledge in a Neural Network https://arxiv.org/abs/1503.02531]


	[Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer]
		Student models learn [attention] from teacher models

	[Born Again Neural Networks]
 	Student performs better when the same model is used by teacher and student

	[Deep Mutual Learning]
 	Students teach each other.
