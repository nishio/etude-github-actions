Toward Controlled Generation of Text
Sentence Generation Task with Neural Networks
Generator is [LSTM-RNN].
The hidden variable z is sampled randomly from the probability distribution output by Encoder
[VAE]:[Variable Auto Encoder].
Discriminator takes a sentence as input and makes c
Blue dotted line is the independent constraint proposed in the second half
Red arrows indicate backpropagation made possible by variational approximation
[GAN]-like configuration

[https://gyazo.com/a10a9ee3a963b880f7470373c273ddb5]

Learning algorithm alternates between Generator and Discriminator
[https://gyazo.com/c2129280c86d21571c7e765c095b00d9]

>Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P. Xing
>(Submitted on 2 Mar 2017 (v1), last revised 13 Sep 2018 (this version, v4))
>Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.
