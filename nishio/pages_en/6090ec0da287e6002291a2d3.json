Hatena2014-01-10
code:hatena
 <body>
 *1389342744*Trying to use Logistic Regression in sklearn
 I made 2000 Logistic Regression of 400 features, L1 regularized, and there are 62874 features that are not zero. This means that there are 31 per feature, which means that 90% of the features are garbage.
 
 But this is a "let's try it with simple features for now", so the performance is worse than the naive Bayesian judge I made before. Hmmm. I'll bring the na√Øve Bayesian features to this one and try it out.
 
 Each Logistic Regression uses 10,000 400 dimensional data to fit, 37 minutes in total, and each training takes roughly 1-2 seconds.
 
 There was nothing particularly difficult about learning or predicting, as the explanation says so. <a href='http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html'>sklearn.linear_model. LogisticRegression &#8212; scikit-learn 0.14 documentation</a>.
 
 The important knowledge that is not written here is that it is OK to save the data of the learner that was created by writing this 37 minutes by joblib.dump(lr, 'filename'). from sklearn.externals import joblib.
 
 <hr>
 
 After adding the features, the 400 dimensional vectors shared by 2000 LRs in the previous experiment increased to 912 dimensional * 2000 vectors, which is hard to do on a MacBook Air. Changed to sparse matrix.
 </body>

[Hatena Diary 2014-01-10 https://nishiohirokazu.hatenadiary.org/archive/2014/01/10]