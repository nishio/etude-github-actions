Transformer
A report that Transformer composed only of [Attention Mechanism] without [RNN] and without [CNN] performs well in translation tasks.
>We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. 

	Attention Is All You Need [≈Åukasz Kaiser et al., arXiv, 2017/06]
	2017
	https://arxiv.org/pdf/1706.03762.pdf 


Commentary 2017-12 http://deeplearning.hatenablog.com/entry/transformer
	[Attention mechanism]
 	[Attention mechanism is a dictionary object].
 	[additive attention] and [inner product attention].
  [Source Target Attention] and [Self Attention].
  [Scaled Dot-Product Attention

