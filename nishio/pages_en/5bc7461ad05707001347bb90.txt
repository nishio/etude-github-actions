Sequence-to-Sequence Model
	Also [seq2seq], [Encoder-Decoder], [Series Transformation Model].
 [1409.3215 Sequence to Sequence Learning with Neural Networks https://arxiv.org/abs/1409.3215](2014)
	If you use the [self-note] mechanism, isn't there much need to use the Encoder-Decoder configuration? I'm starting to get the feeling that (2018-10-17)


	[Pointer Networks]
		[1506.03134 Pointer Networks https://arxiv.org/abs/1506.03134](2015)
		It is possible to use the pointer to copy from the input
		[CopyNet]
   [1603.06393 Incorporating Copying Mechanism in Sequence-to-Sequence Learning https://arxiv.org/abs/1603.06393]
		[Pointer Sentinel Mixture Models]
  	[1609.07843 Pointer Sentinel Mixture Models https://arxiv.org/abs/1609.07843]
		[Pointer-Generator Network] (2017)
			[1704.04368 Get To The Point: Summarization with Pointer-Generator Networks https://arxiv.org/abs/1704.04368]
https://qiita.com/ymym3412/items/c84e6254de89c9952c55
