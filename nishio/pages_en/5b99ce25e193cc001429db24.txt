Similarity of concepts is not distance.
I wrote this in [Natural Language Processing with word2vec] p.64
>Apples and tomatoes are similar. Both are red. Apples and green apples are similar. Both are apples. However, green apples and tomatoes are not so similar.

This phenomenon does not satisfy the [triangle inequality] requirement for [distance].

How to solve this problem
	Instead of treating the distance between vectors and [vector similarity] as it is as semantic similarity, make the distance after collapsing it on various axes as similarity #Crush axes
[https://gyazo.com/6f605fd9a0082f691b3b93c575ccd69e]

	To [collapse] a vector in an axial direction means to [ignore] differences in that axial direction.
 [dimensionality reduction is an abstraction] #abstraction #ignore features #ignore differences
	I doubt that one axis of the vector created by the current [word2vec] represents a convenient attribute like "color difference".
		word2vec creates vectors based solely on the information of what words appear around a word, so it is not possible to create vectors based on the information of what words appear around a word.
	I think something similar is going on in the human brain.
 One of the techniques used in [Deep Learning] is [Dropout
 	A method of randomly selecting [neurons] and stopping their activity to allow them to learn
  Doing this increases [generalization performance].
  Stop the activity of randomly selected neurons
  	= set the value represented by that neuron to 0
   = Crush in the direction of a randomly selected axis

[Similarity] of [concept] is not [distance
[word meaning].
[Similarity of meaning]

[who leapfrogs the conversation = vector search engine?]
	Is [associative] a vector search? I think not.
　　Random [dimensionality reduction and then similarity search].