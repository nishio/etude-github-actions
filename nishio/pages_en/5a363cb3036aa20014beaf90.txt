Geister Algorithm Explained
	Geister AI using Partial Observation Monte Carlo Programming
  Cybozu Labs Yasukazu Nishio
 Purpose of this paper
 	A brief description of what's in the AI that was submitted to and won second place in the Guyster AI competition at GPW in November 2017 will facilitate research on how to build AI in situations where inference of unobservable information is important.
  http://www2.matsue-ct.ac.jp/home/hashimoto/geister/
	Pitfalls of thinking in discrete logic
  For example, let's say there is a rock-paper-scissors game in which players get one point if they buy with goo, and two points if they buy with choki and par. How can we create a strong AI for this game?
  As a first step, let's say that your opponent has a 1/3 chance of choosing your move. In such a case, the expected point difference is the highest if you choose to play choki. If you win, you get 2 points, and if your opponent wins, he gets 1 point.
  Then, is the best strategy "a strategy to play choki"? As you can see, that strategy loses out to the "goo strategy. That strategy loses to the "par strategy. And that strategy loses to the strategy of making a "Choki". You think you are thinking logically, but your thoughts go in circles, and you give up creating AI.
		In this game, there are three possible action options: goo, choki, and par. If we consider the strategy of this game as "a function that takes a situation as an argument and returns one of the three actions," we will be stuck in the roundabout way described above. Instead, we need to extend it to "a function that takes a situation as an argument and returns a three-dimensional vector of real values. The return value of this function is assumed to be the sum of 1. It is a discrete probability distribution. Within this framework, the return value for "a strategy that produces goo" is (1, 0, 0), and the return value for "a strategy that chooses each option with a probability of 1/3" is (1/3, 1/3, 1/3).
  The framework, in which action alternatives are chosen stochastically, is a "mixed strategy game" in the language of game theory. Nash's theorem proves that there exists at least one Nash equilibrium in a mixed strategy game. A Nash equilibrium is "a combination of strategies in which no player can obtain a higher gain by changing his or her strategy. If the Geister AI becomes strong enough, it will reach this Nash equilibrium.
 partially observed Markov decision process
 	If, in a certain board state s, I take action a, how can I describe the board state s' when it is my turn next?
  In the field of reinforcement learning, this is considered a Markov decision process. In other words, the transition from one state s to another state s' is probabilistic according to the transition probability P(s, a, s').
  This "board state" is not the board as seen by you. It includes hidden information that only the opponent can see. This is because the probability that the opponent chooses each action is affected by information that only the opponent can see. In other words, Geister is a Markov decision process, but one player cannot observe all of the states. This is called a partially observed Markov decision process. This is a problem setting that is often studied in the field of reinforcement learning.
  The information that cannot be observed by one player is "which of the 70 possible initial placements of the opponent's pieces was 8C4". If we express which we think it was by a real number between 0 and 1, it becomes a 70-dimensional vector of real numbers. This is called the "belief" in the other's initial placement. This vector has a sum of 1 and can be thought of as a discrete probability distribution.
  Many people mistakenly believe that the "state" of a Markov decision process is a discrete set. If you imagine a robot's attitude control, you will see that the state can be continuous in general. Therefore, let us include a 70-dimensional real-valued belief vector as a state. In a partially observed Markov decision process, by including beliefs about unobservable information in the state as a probability distribution, we can create a new fully observed Markov decision process. This is called belief MDP.
  The partial problem of "estimating the opponent's color" in Geister can be thought of as a Bayesian estimation process in which the prior distribution of beliefs is updated by observing the opponent's behavioral information. As a concrete example, consider the case where you observe that "the opponent's piece is just before the goal, but in the opponent's turn, the opponent did not score a goal. Think about what you would do if you were in your opponent's position. Since you did not know in advance whether the piece was red or blue, assume a 50% chance of both. If the piece is blue, I would have a 25% chance of not scoring a goal, which you may think is 0%, but this is for the sake of explanation. If the piece is red, you cannot finish the goal, so you will not finish 100% of the time. The observed fact was "no goal". So, the observed fact is reproduced with a probability of 50%×25% + 50%×100%. The 50% x 25% case is blue. In other words, the belief in this frame is updated from 50% blue and 50% red to 20% blue and 80% red by this observation.
  Since the observation of the goal line is extreme in probability, a person thinking in terms of logic could have similarly deduced that "this frame is red". In reality, ambiguous observations are observed, such as "When I pressed him to exchange the frames, he ran away," which cannot be confirmed to be blue, but cannot be said to be uninformed either. The game of Geister is to gradually seize the opponent's secret information from these observations and build up an advantageous position.
 Partial Observation Monte Carlo Programming
 	Geister is not only a partially observed Markov decision process, but also a tricky problem where the state transition probabilities are not explicitly given. Partially observed Monte Carlo programming can be used in such situations. Instead of state transition probabilities, a black box simulator that can be run repeatedly is given to solve the problem in Monte Carlo.
		Partial Observation Monte Carlo programming is a combination of particle filtering (also known as Sequential Monte Carlo) and Monte Carlo tree search. The particle filter updates the belief distribution. Specifically, the state is sampled from the belief distribution, and the simulator assumes that the state is the same as the belief distribution and allows the opponent's move to advance by one move. The remaining samples are discarded, leaving only those that match the opponent's actual move, resulting in a new belief distribution that has been updated by the observation of the opponent's move.
  The Monte Carlo tree search part assumes a state sampled from the belief distribution and selects moves according to an appropriate rollout policy from that state to obtain information on which moves have a high winning rate. This information is accumulated in a tree structure, and when a certain amount of information has been gathered, moves are selected using another Tree Policy (the famous UCB1).
  The author implemented this Partial Observation Monte Carlo Programming method in this contest. Particle Reinvigoration, described below, was not implemented due to lack of time. In addition, a model of the opponent AI is necessary in the implementation of the simulator. There are two versions of this model: the version with the algorithm Fastest (POMCP-Fastest), which "aims to reach the goal in blue anyway," and the version with the algorithm Ichi (POMCP-Ichi), which does not anticipate any move, and whose value function is constructed by craftsmanship by combining if statements based on Nishio's sensibilities ( POMCP-Ichi).
 Results and Discussion
 	Unfortunately, the author did not understand that the tournament was to be held as part of a workshop requiring advance registration, and by the time he realized it, registration had closed, so he sent only the program to play against us. A round-robin competition of all AIs was held, and the author's POMCP-Ichi and Naoto Kawakami's Naocchi MINMAX won all the games against the other participants and tied each other with one win and one loss against each other. The two AIs played two rounds of playoffs to determine the winner, with POMCP-Ichi winning the second place with two defeats.
  I guess from the name that Naochi MINMAX uses the MinMax method, which is a "worst-case best-case move" method, and is very risk-averse and cautious in nature. On the other hand, the author's POMCP does not implement Particle Reinvigoration by cutting corners. Let's consider. The particle filter updates beliefs by sampling from the belief state and rejecting a portion of it. Even if there is no difference in the rejected part, the strength of beliefs will randomly walk due to luck of sampling. And if by bad luck the particle goes to zero, the belief is never sampled again. It is the strong nature of the belief that if one believes, "It must not be X," one will never again consider the possibility that it is X. Particle Reinvigoration mitigates this.
  In fact, this was a known problem. In a user test conducted only once due to time constraints, POMCP-Fastest pushed his opponent, Ikuo Takeuchi, to a bet of two choices, one blue and one red, but as the two players glanced at each other before the goal, he thought, "This panel that is not advancing must be red," and left the goal, losing easily. The red one was easily defeated. This was also the reason why I had to prepare Ichi, which was more decent than Fastest, in a hurry.
 summary
 	He explained "beliefs," a useful concept for Geister's AI, and partial observation Monte Carlo programming, a useful algorithm for estimating them. We hope this information will make future Geister AI competitions more interesting.
  The author is thinking of competing next time with POMCP-MinMax after implementing Particle Reinvigoration. He also predicts that a beneficial AI will occupy a very small subspace of the total space of possible AIs, and he thinks it would be possible to estimate the algorithm of the opponent, just as it would be possible to estimate the initial placement of the red frames. The author is convinced from his own experience against human opponents that side-channel attacks, which steal information from thinking time, are effective when the opponent is human. Also, the current POMCP does not contain any optimization to reduce information leakage from one's actions. Since the publication of this article has increased the probability of POMCP coming to the opponent, there is a way to create POMCP-POMCP to counter it. And POMCP-(POMCP-POMCP). If we repeat this infinitely, we will probably reach Nash equilibrium.
