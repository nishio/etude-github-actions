Limit the size of memory
	The cost of searching for points in the neighborhood is O(N^2)
		Humans may have hardware solutions.
	Instead of making a similarity determination from all past data, we will only start with the most recent input.
		That would make it O(N).
	Even those that are not the most recent input are rarely successfully linked, but I'm not sure why that is.
		Past memories are picked up in dreams, etc. and piled into the "most recent memory".
			[Incremental Reading]-like [Interval Iteration Method] algorithms are fine.
	Limiting memory to reasonable values will limit the amount of computation.

First published: 2019-12-17, edited slightly from [subjectivity and emotion#5da876a8aff09e0000cc6eb2].
	I found it by searching for the term "[memory size limit]."
