Hatena2015-06-27
code:hatena
 <body>
 *1435371890*Pyspa Diary
 We began to build a language model for Python.
 2038 samples, average number of tokens per sample 10491, maximum number of tokens 290226.
 I have a pretty long token string, so a fixed n-gram seems like a waste.
 
 There are 135 token species, 21380862 tokens in all.
 
 Number of occurrences by token (partial)
 >||
 NAME 1086490
 factor 918992
 atom 911487
 power 911487
 term 897816
 arith_expr 885288
 shift_expr 884911
 and_expr 884475
 xor_expr 884412
 ||<
 
 LogLoss with 256 hidden layers was 82.29 after 10000 cases, 82.33 in the test. it has been running loosely since then, about 82.35.
 
 1000 mini batches
 train mean loss=71.2530914678
 test  mean loss=71.2531124351
 
 I thought it was awfully big, but with two tokens the probability is almost 1. What is wrong?
 </body>

[Hatena Diary 2015-06-27 https://nishiohirokazu.hatenadiary.org/archive/2015/06/27]