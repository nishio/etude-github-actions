Hatena2014-02-10
code:hatena
 <body>
 *1392026316*Deep Learning論文紹介「Deep learning via Hessian-free optimization」その3
 We regret to inform you that this series of articles will not continue.
 
 So to begin with, my interest was in understanding meaning with word2vec and generating sentences with language models. The latter paper used RNNLM, which is what the author of the former paper was doing before word2vec. So I started looking into RNNLM, and then I was looking into Hessian-free optimization, because its optimization was difficult to do with stochastic gradient methods, which are commonly used in other neural nets.
 
 As I explained <a href='http://d.hatena.ne.jp/nishiohirokazu/20140209/1391874480'>last time</a>, it is true that if a neural net has neurons with almost identical weights, the gradient in the direction of differentiation of the neurons is too small for the first-order gradient method to learn. I understand that if there are neurons with almost identical weights in a neural net, the gradient in the direction of differentiation of the neurons is too small for the first-order gradient method to learn well. But I don't really agree with the conclusion that "that's why we use second-order optimization methods". Personally, I think, "If you apply sparsification constraints, differentiation will occur quickly due to the fact that only one of the neurons can fire. Even if we want something that is not sparse in the end, why don't we start with sparse constraints and look for "good initial values"? Since "that's good" or "that's better" is just a hypothesis, I think it would be interesting to test whether the hypothesis is correct or not, but I'm actually not very interested in it.
 
 One more point, I don't want to use morphological analysis where I make a list of words to feed to word2vec. So I was looking for research on discovering "words" from RNNLMs, and found a description of a method that rather "provides almost the same accuracy as results from deep learning with minimal parameter adjustment" (<a href='http://d.hatena.ne.jp/ mamoruk/20130913/p1'>SIGNL 213: realistic morphological analyzer input -> "zumomomo pero pero pero mamitas mamitas love mamitas" - Musashino Diary</a>)
 
 So, since my interest has shifted to this paper, the series is over: http://chasen.org/~daiti-m/paper/nl190segment.pdf
 
 *1392040581*Re: Tokyo is a hard place to live
 I do not agree with the title at all, but the white dashi sent from my parents' house in Osaka is very useful for my wife. Udon noodles eaten at home are good and I am happy (NOROKE)
 
 ref. bookworm: Tokyo is Hard to Live in http://cpplover.blogspot.jp/2014/02/blog-post_10.html
 </body>

[Hatena Diary 2014-02-10 https://nishiohirokazu.hatenadiary.org/archive/2014/02/10]