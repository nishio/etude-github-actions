Critique of p-value
	2014 Nuzzo, R.(2014). Scientific method:statistical errors. Nature, 506, 150-152
 	https://www.nature.com/news/scientific-method-statistical-errors-1.14700
  p-values were created by Fisher, but were not the purpose of the test
  Neyman et al. devised the test mechanism.
  While Fisher and Neyman were criticizing each other's approach, other authors mixed up their arguments and a "method of testing by calculating a p-value and comparing it to a threshold value (such as 0.05)" was born.
  How the distribution is updated before and after observing a result that is 0.05 significant when tested ([Bayesian] interpretation)
  	If the prior distribution has a 5% probability of being a real effect, the probability only increases to 11% if the test is found to be significant.
   If the prior distribution is 50-50, it rises to 71% after the test is determined to be significant.
   The common interpretation "If a phenomenon is significant when tested at a 5% level of significance, there is a 95% probability that the phenomenon is real" is incorrect.

	2015 Basic and Applied Social Psychology（BASP）
		[Prohibit the presentation of null hypothesis significance test methods and p-values within the journal https://www.tandfonline.com/doi/full/10.1080/01973533.2015.1012991]
  Nature News on it [Nature News https://www.nature.com/news/psychology-journal-bans-p-values-1.17001]

 2016 American Statistical Association（ASA）
 	[Statement on p-values https://amstat.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108]
  There are a lot of things, but I'll pick some of them up.
 	 The p-value is not a measure of the probability that the hypothesis is true or that the data were generated by chance
  	Scientific conclusions and business or policy decisions should not depend solely on whether the p-value passes a particular threshold
	  The p-value or statistical significance is not a measure of the size of the effect or the importance of the result
  	A p-value by itself does not provide good evidence for a model or hypothesis
		[ASA (American Statistical Association), annoyed by misuse of p-values, issues statement | SciStat http://epigraph.jugem.jp/?eid=253]

	2016 "Recent Discussions on p-values" ([Japanese commentary https://www.jstage.jst.go.jp/article/jscssymo/30/0/30_153/_pdf/-char/ja])

memo
	http://smrmkt.hatenablog.jp/entry/2015/04/14/234856
 	[Optimizely]
  [Bayes]
  The sample size increases over time, but it is unnatural to not look at the data until a defined sample size is reached.
  Repeated viewing increases the false positive rate.
  [Sequential Test] Base
  From [False Positive Rate] to [False Detection Rate

	[Tests and Interval Estimation https://oku.edu.mie-u.ac.jp/~okumura/stat/tests_and_CI.html]
		>It is difficult to say what to do, but since [confidence intervals are more important than p-values] and [Clopper and Pearson's] method for obtaining [confidence intervals] has already become the standard, how about obtaining confidence intervals using it and not reporting the p-values in particular (let the respondents judge whether the confidence intervals include the number of null hypotheses)? How about we do not report the p-value in particular?
