Chain of Thought
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
	We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.
	Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.
	https://arxiv.org/pdf/2201.11903.pdf
(DeepL)Thought Chain Prompting for Eliciting Inference in Large Language Models
	We explore how the generation of chains of thoughts (a series of intermediate inference steps) can significantly improve the ability of large language models to perform complex inferences. In particular, we show that such reasoning abilities emerge naturally in sufficiently large language models by a simple method called thought chain prompting, in which several thought chain demonstrations are provided as exemplars for the prompts.
	Experiments with three large-scale language models show that thought chain prompts improve performance on arithmetic, common sense, and symbolic reasoning tasks. The empirical improvements are significant. For example, by providing the PaLM 540B with only 8 thought-chain prompts, it achieved state-of-the-art accuracy on the GSM8K benchmark for math word problems, outperforming even the GPT-3, which was fine-tuned using a validator.

[https://gyazo.com/f21f8aafa600f51e984c7de12c2a3d7e]
[LangChain Glossary]