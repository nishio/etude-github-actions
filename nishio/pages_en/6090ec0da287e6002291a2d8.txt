Hatena2014-01-28
code:hatena
 <body>
 *1390901484*AIC confirmed experimentally with linear regression.
 The more parameters used in the prediction, the better the fit to the training data, even if the data is just noise and does not appear in the true model at all. But of course, the fit to the test data will be worse.
 
 So I experimented with a model that uses linear regression to predict the correct answer t from 1 to 5 parameters, where t is the first parameter plus noise, and the second to fourth parameters are completely unaffected. However, the log-likelihood for the training data still increases by about 0.5 for each additional parameter used in the prediction.
 
 >[ 0.          0.49161953  0.99283812  1.47318384  1.9262704 ]
  
 On the other hand, the log likelihood for the test data decreases by approximately 0.5 for each additional parameter.
 
 >[ 0.         -0.55409662 -1.11481264 -1.66386383 -2.08548524]
 
 So, for each additional parameter, the difference increases by 1. So, in AIC, maximizing the log likelihood minus the number of parameters from the log likelihood for the training data is estimating and maximizing the log likelihood for the test data.
 
 Source : https://gist.github.com/nishio/8595089
 </body>

[Hatena Diary 2014-01-28 https://nishiohirokazu.hatenadiary.org/archive/2014/01/28]