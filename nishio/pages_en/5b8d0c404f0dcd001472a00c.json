Implement PCA without PCA
code:python
 import numpy as np
 # Prepare appropriate data 5 dimensions 6 cases of data
	data = np.random.random((6, 5))
 # Subtract average
	d2 = data - data.mean(axis=0)
 # 共分散行列を作る
	cov = np.einsum("ij,ik->jk", d2, d2)
	
 # The covariance matrix is a square matrix of the dimensions of the data
	>>> cov.shape
	(5, 5)
	
	# 共分散行列を固有値分解する
	>>> np.linalg.eig(cov)
 (array([1.34012189, 0.77663231, 0.58201555, 0.08694709, 0.03991043]),
  array([[ 0.67869952,  0.35340645, -0.37436676,  0.50602025,  0.13514392],
         [ 0.45792096,  0.20519401,  0.77490418, -0.08403411, -0.37505412],
         [-0.46695028,  0.17178326,  0.04681577,  0.68316922, -0.53248104],
         [-0.26671759,  0.89310368, -0.08178549, -0.34559589,  0.07142937],
         [-0.20123254,  0.07652205,  0.50049221,  0.38823327,  0.74325791]]))
 
 # 比較対象のPCA実装
 from sklearn.decomposition import PCA
 # Fit to data
 m = PCA().fit(data)
	
 # 主成分軸を表示 
 >>> m.components_
 array([[ 0.67869952,  0.45792096, -0.46695028, -0.26671759, -0.20123254],
         [ 0.35340645,  0.20519401,  0.17178326,  0.89310368,  0.07652205],
         [-0.37436676,  0.77490418,  0.04681577, -0.08178549,  0.50049221],
         [ 0.50602025, -0.08403411,  0.68316922, -0.34559589,  0.38823327],
         [ 0.13514392, -0.37505412, -0.53248104,  0.07142937,  0.74325791]])
	
 # Comparing the results of the eigenvalue decomposition of the covariance matrix with the principal component axes of the PCA, the difference is almost zero
 >>> np.linalg.eig(cov)[1] - m.components_.T
 array([[-4.44089210e-16,  1.11022302e-16,  4.99600361e-16,
          1.11022302e-16,  1.66533454e-16],
        [-2.22044605e-16, -6.66133815e-16,  4.44089210e-16,
         -3.88578059e-16, -1.11022302e-16],
        [ 1.11022302e-16, -3.05311332e-16, -2.56739074e-16,
         -3.33066907e-16,  3.33066907e-16],
        [-2.22044605e-16,  0.00000000e+00,  2.08166817e-15,
          3.33066907e-16, -3.60822483e-16],
        [ 2.22044605e-16, -1.02695630e-15, -1.11022302e-16,
          6.66133815e-16,  0.00000000e+00]])
	
 # Relationship to Singular Value Classification (SVD) (same except that some of the signs on the axes are reversed)
 >>> -np.linalg.svd(d2)[2]
 array([[ 0.67869952,  0.45792096, -0.46695028, -0.26671759, -0.20123254],
        [ 0.35340645,  0.20519401,  0.17178326,  0.89310368,  0.07652205],
        [-0.37436676,  0.77490418,  0.04681577, -0.08178549,  0.50049221],
        [-0.50602025,  0.08403411, -0.68316922,  0.34559589, -0.38823327],
        [-0.13514392,  0.37505412,  0.53248104, -0.07142937, -0.74325791]])
 
 >>> m.components_
 array([[ 0.67869952,  0.45792096, -0.46695028, -0.26671759, -0.20123254],
        [ 0.35340645,  0.20519401,  0.17178326,  0.89310368,  0.07652205],
        [-0.37436676,  0.77490418,  0.04681577, -0.08178549,  0.50049221],
        [ 0.50602025, -0.08403411,  0.68316922, -0.34559589,  0.38823327],
        [ 0.13514392, -0.37505412, -0.53248104,  0.07142937,  0.74325791]])

#主成分分析 #特異値分解 #共分散行列 #PCA

 [共分散行列]を作る
	 The covariance matrix is a square matrix of the dimensions of the data
	共分散行列を[固有値分解]する
	 [PCA]の[主成分軸]を表示 
	Comparing the results of the eigenvalue decomposition of the covariance matrix with the principal component axis of the PCA, the difference is almost zero
 Relationship to [Singular Value Classification]([SVD]) (identical except that the signs of some of the axes are reversed)