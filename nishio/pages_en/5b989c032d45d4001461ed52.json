劣微分
[FOBOS]
[劣微分を用いた最適化手法について(4) | Preferred Research https://research.preferred.jp/2010/12/subgradient-optimization-4/]

>At the end of the last issue, I wrote that [L1 regularization] has a strong effect of reducing the nonzero elements of the parameters, but that [SGD] cannot calculate them in the first place because they are non-differentiable at the position wi = 0. ...... But when you think about it, don't you think that you can take any appropriate value between -1 and 1 as the value of the derivative...? The [Subordinate Differentiation] answers that question.
[劣微分を用いた最適化手法について(3) | Preferred Research https://research.preferred.jp/2010/12/subgradient-optimization-3/]

>[L2 regularization], the penalty is almost zero if the value of wi is close to zero, while for L1 regularization, the penalty is zero only if the value is exactly zero. Therefore, L1 has more power to reduce the non-zero elements of the parameter vector. As written, L1 regularization looks better, but it is [not differentiable] at 0, which makes it difficult to use general numerical optimization methods. So, various studies were born to realize L1 regularization.
	[劣微分を用いた最適化手法について(2) | Preferred Research https://research.preferred.jp/2010/11/subgradient-optimization-2/]

	[劣微分を用いた最適化手法について(1) | Preferred Research https://research.preferred.jp/2010/11/subgradient-optimization-1/]

[Cumurative Penalty]
	[劣微分を用いた最適化手法について(完) | Preferred Research https://research.preferred.jp/2011/02/subgradient-optimization-5/]

Linear regression mixing L1 and L2 is called [ElasticNet
http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html
